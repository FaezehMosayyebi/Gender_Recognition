{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Amf88OMgzZ2"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.unpack_archive('/content/dataset.zip')"
      ],
      "metadata": {
        "id": "FA4ILS-DD0C7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import imghdr\n",
        "\n",
        "data_dir = \"/content/dataset/male\"\n",
        "image_extensions = [\".png\", \".jpg\"]  # add there all your images file extensions\n",
        "\n",
        "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
        "for filepath in Path(data_dir).rglob(\"*\"):\n",
        "    if filepath.suffix.lower() in image_extensions:\n",
        "        img_type = imghdr.what(filepath)\n",
        "        if img_type is None:\n",
        "            print(f\"{filepath} is not an image\")\n",
        "        elif img_type not in img_type_accepted_by_tf:\n",
        "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")"
      ],
      "metadata": {
        "id": "2UwEdjioURTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup Network\n",
        "from main import pipeline\n",
        "\n",
        "# @markdown ### Enter model path:\n",
        "model_direction = '/content/save/genderdetector.h5' # @param {type:\"string\"}\n",
        "if model_direction == '':\n",
        "  model_direction = None\n",
        "pip = pipeline(model_direction)\n",
        "\n",
        "# @markdown ### You don't have a model! Leave the above box empty and train one for yourself :)\n",
        "train_dataset = '/content/dataset' #@param{type:\"string\"}\n",
        "validation_dataset = '/content/dataset' #@param{type:\"string\"}\n",
        "data_dir = {'training': train_dataset,\n",
        "            'validation': validation_dataset}\n",
        "save_to = '/content/save' #@param{type:\"string\"}\n",
        "\n",
        "# @markdown ### Enter image size:\n",
        "batch_size = 2 # @param {type:\"integer\"}\n",
        "patch_size = (240, 240)  # @param {type:\"raw\"}\n",
        "# @markdown Note: For EfficientNet it should be (240,240).\n",
        "learning_rate = 0.001 # @param {type:\"slider\", min:0, max:0.1, step:0.0001}\n",
        "\n",
        "# @markdown ### We have two type of models. Please select one.\n",
        "\n",
        "model_type = 'Small' # @param ['Small', 'Large']\n",
        "\n",
        "training_epochs_number = 4 # @param {type:\"integer\"}\n",
        "# @markdown ### Do you want to fine tune the model?\n",
        "tune_from = 84 # @param {type:\"slider\", min:0, max:339, step:1}\n",
        "tuning_epochs_number = 5 # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown ### Do you want to Augment your data?\n",
        "augmentation = True # @param {type:\"boolean\"}\n",
        "rotation_factor = 0.52 # @param {type:\"slider\", min:-6.28, max:6.28, step:0.1}\n",
        "translation_height_factor = 0.2  # @param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "translation_width_factor = 0.2  # @param {type:\"slider\", min:-1, max:1, step:0.1}\n",
        "flip = \"horizontal\" # @param [\"horizontal\", \"vertical\", \"horizontal_and_vertical\"]\n",
        "upper_contrast_factor = 0.1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "lower_contrast_factor = 0.1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "aug_config={'rotation_factor' : rotation_factor,\n",
        "            'translation_factor' : (translation_height_factor, translation_width_factor),\n",
        "            'flip': flip,\n",
        "            'contrast_factor' : (upper_contrast_factor, lower_contrast_factor),\n",
        "}\n",
        "\n",
        "if model_direction == None:\n",
        "  pip.train(data_dir, model_type, batch_size, patch_size, training_epochs_number, tuning_epochs_number, tune_from, learning_rate, augmentation, aug_config, save_to)\n"
      ],
      "metadata": {
        "id": "acfe_dOwCA3S",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utilize Model\n",
        "image_direction = '/content/dataset/female/1.jpg' #@param{type:\"string\"}\n",
        "save_to_dir = True #@param{type:\"boolean\"}\n",
        "save_path = '/content/save' #@param{type:\"string\"}\n",
        "pip.run_model(image_direction, save_to_dir, save_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6wUbS0HMh8eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Evaluation\n",
        "\n",
        "# @markdown ### Enter a test dataset directory:\n",
        "test_dataset_directory = '/content/dataset' # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ### Enter image size:\n",
        "# @markdown Note: For EfficientNet B1 it should be (240, 240)\n",
        "patch_size = (240, 240)  # @param {type:\"raw\"}\n",
        "\n",
        "# @markdown ### Do you want to see the confusion matrix?:\n",
        "confusion_matrix =False # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "pip.evaluate_model(test_dataset_directory, batch_size, patch_size, confusion_matrix)"
      ],
      "metadata": {
        "id": "HiQmKid9BZ8a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Face Detector\n",
        "#@markdown If you want to flow from directory insert source directory path in the following box.\n",
        "image_path = '/content/dataset/female' #@param{type:\"string\"}\n",
        "#@markdown Do you want to save the result?\n",
        "save_to_dir = True #@param {type:\"boolean\"}\n",
        "flow_from_directory = True #@param {type:\"boolean\"}\n",
        "#@markdown If you want to flow from directory insert destination directory path in the following box.\n",
        "destination_directory = '/content/save' #@param {type:\"string\"}\n",
        "#@markdown If flow from directory insert your preferred prefix.\n",
        "prefix = 'faezeh' #@param{type:\"string\"}\n",
        "\n",
        "\n",
        "pip.detect_faces(image_path, save_to_dir,destination_directory, flow_from_directory, prefix)"
      ],
      "metadata": {
        "id": "61f7RTN-hnLc",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Face Landmarks Detection\n",
        "#@markdown The number of landmark you want to be detected\n",
        "num_landmark = \"68\" # @param [5,68]\n",
        "#@markdown If you want to flow from the directory, insert the source directory path in the following box.\n",
        "image_path = '/content/dataset/female/1.jpg' #@param{type:\"string\"}\n",
        "#@markdown Do you want to save the result?\n",
        "save_to_dir = False #@param {type:\"boolean\"}\n",
        "flow_from_directory = False #@param {type:\"boolean\"}\n",
        "#@markdown If you want to flow from the directory, insert the destination directory path in the following box.\n",
        "destination_directory = '' #@param {type:\"string\"}\n",
        "#@markdown If flow from directory insert your preferred prefix.\n",
        "prefix = '' #@param{type:\"string\"}\n",
        "\n",
        "pip.detect_face_landmarks(image_path, int(num_landmark), save_to_dir, destination_directory, flow_from_directory, prefix)"
      ],
      "metadata": {
        "id": "oV-wRNQGo0E9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "99a4889d-d89a-459a-beb5-65b34f4a2854"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-22515943548f>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;31m#@param{type:\"string\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_face_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_landmark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_to_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflow_from_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/main.py\u001b[0m in \u001b[0;36mdetect_face_landmarks\u001b[0;34m(self, image_dir, num_landmark, save_to_dir, destination_directory, flow_from_directory, prefix)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0myou\u001b[0m \u001b[0mwant\u001b[0m \u001b[0mto\u001b[0m \u001b[0msave\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mlandmark_detector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLandmakDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_landmark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mflow_from_directory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/landmark_detection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, landmark_number)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mlandmarks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/shape_predictor_68_face_landmarks.dat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlandmark_predictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshape_to_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to open shape_predictor_68_face_landmarks.dat"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Face Masking\n",
        "#@markdown **Poisson image editing thresholds**\n",
        "low_threshold = 100 # @param {type:\"slider\", min:0, max:100, step:1}\n",
        "high_threshold = 2560 # @param {type:\"slider\", min:100, max:10000, step:50}\n",
        "#@markdown **The size of the Sobel kernel to be used**\n",
        "kernel_size = 36 # @param {type:\"slider\", min:3, max:100, step:1}\n",
        "#@markdown **If you want to flow from the directory, insert the source directory path in the following box.**\n",
        "image_path = '' #@param{type:\"string\"}\n",
        "#@markdown Do you want to save the result?\n",
        "save_to_dir = True #@param {type:\"boolean\"}\n",
        "flow_from_directory = True #@param {type:\"boolean\"}\n",
        "#@markdown If you want to flow from the directory, insert the destination directory path in the following box.\n",
        "destination_directory = '' #@param {type:\"string\"}\n",
        "#@markdown If flow from directory insert your preferred prefix.\n",
        "prefix = '' #@param{type:\"string\"}\n",
        "\n",
        "pip.generate_masked_faces(low_threshold, high_threshold, kernel_size, image_dir, save_to_dir, save_directory, flow_from_directory, prefix)"
      ],
      "metadata": {
        "id": "OffAp6H0Rrpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}